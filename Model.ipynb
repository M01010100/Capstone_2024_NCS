{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model.ipynb \n",
    "A proof of use for Homomorphic Encryption. \n",
    "Code was created over several weeks of trial and error, different libraries and implementations. Began with sklearn, moved to torch. For FHE I ended up on Tenseal but I did have previous iterations with Conrete-ml by Zama AI and Phailiar cryptosystems.\n",
    "\n",
    "# Problems\n",
    "Specifically with the TenSEAL Implementation, which I decided to limit this capstone to along with the CKKS scheme. First it was the encrypted data was cast as a CKKS.Vector. The current release for most ML libraries do not have models that can use data in the 'scheme'.vector form. Was forced to write sections which would be predefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tenseal as ts\n",
    "import pandas as pd\n",
    "import random\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Data summary #############\n",
      "x_train has shape: torch.Size([896, 4])\n",
      "y_train has shape: torch.Size([896, 1])\n",
      "x_test has shape: torch.Size([224, 4])\n",
      "y_test has shape: torch.Size([224, 1])\n",
      "#######################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matt\\AppData\\Local\\Temp\\ipykernel_21276\\2342525001.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = grouped.apply(lambda x: x.sample(grouped.size().min(), random_state=13).reset_index(drop=True))\n"
     ]
    }
   ],
   "source": [
    "# Set the random seeds for reproducibility\n",
    "torch.random.manual_seed(13)\n",
    "random.seed(13)\n",
    "\n",
    "# Create indicies list, shuffles indices at random rate, and splits data into training and testing data\n",
    "# meant to increase the randomness of the data\n",
    "def split_train_test(x, y, test_ratio=0.2):\n",
    "    idcs = [i for i in range(len(x))]\n",
    "    random.shuffle(idcs)\n",
    "    # delimiter between test and train data\n",
    "    delim = int(len(x) * test_ratio)\n",
    "    test_idcs, train_idcs = idcs[:delim], idcs[delim:]\n",
    "    return x[train_idcs], y[train_idcs], x[test_idcs], y[test_idcs]\n",
    "\n",
    "#*******************************************************************\n",
    "# implementing sklearn's train_test_split\n",
    "# Does only the splitting of the data\n",
    "# Original split_train_test function has syntehtic testing capabilities. \n",
    "#\n",
    "#    def split_train_test(x, y, test_ratio=0.2):\n",
    "#       sklearn.utils.shuffle(x, y)\n",
    "#       x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio)\n",
    "#       return x_train, y_train, x_test, y_test\n",
    "#********************************************************************\n",
    "\n",
    "# Load the data\n",
    "# The data is a credit card fraud dataset, where the goal is to predict whether a transaction is fraudulent or not\n",
    "# The dataset is highly imbalanced, with only 0.17% of the transactions being fraudulent\n",
    "def Credit_data():\n",
    "    data = pd.read_csv(\"payment_fraud.csv\")\n",
    "    # drop some features\n",
    "    data = data.drop(columns=[\"paymentMethod\"])\n",
    "    # balance data\n",
    "    grouped = data.groupby('label')\n",
    "    data = grouped.apply(lambda x: x.sample(grouped.size().min(), random_state=13).reset_index(drop=True))\n",
    "    # extract labels\n",
    "    y = torch.tensor(data[\"label\"].values).float().unsqueeze(1)\n",
    "    data = data.drop(columns=\"label\")\n",
    "    # standardize data\n",
    "    data = (data - data.mean()) / data.std()\n",
    "    x = torch.tensor(data.values).float()\n",
    "    return split_train_test(x, y)\n",
    "\n",
    "# Generate random data to be used for synthetic data for testing\n",
    "def random_data(m=1024, n=2):\n",
    "    # data separable by the line `y = x`\n",
    "    x_train = torch.randn(m, n)\n",
    "    x_test = torch.randn(m // 2, n)\n",
    "    y_train = (x_train[:, 0] >= x_train[:, 1]).float().unsqueeze(0).t()\n",
    "    y_test = (x_test[:, 0] >= x_test[:, 1]).float().unsqueeze(0).t()\n",
    "    return x_train, y_train, x_test, y_test\n",
    "x_train, y_train, x_test, y_test = Credit_data()\n",
    "\n",
    "print(\"############# Data summary #############\")\n",
    "print(f\"x_train has shape: {x_train.shape}\")\n",
    "print(f\"y_train has shape: {y_train.shape}\")\n",
    "print(f\"x_test has shape: {x_test.shape}\")\n",
    "print(f\"y_test has shape: {y_test.shape}\")\n",
    "print(\"#######################################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 0.6125\n",
      "Loss at epoch 2: 0.5438\n",
      "Loss at epoch 3: 0.5030\n",
      "Loss at epoch 4: 0.4783\n",
      "Loss at epoch 5: 0.4623\n",
      "\n",
      "Average time per epoch: 0 seconds\n",
      "Non-Encrypted Accuracy: 0.8036\n"
     ]
    }
   ],
   "source": [
    "# Deining the Logistic Regression torch NN model.\n",
    "class NE_LR(torch.nn.Module):\n",
    "    # n_features is the number of features in the input data    \n",
    "    def __init__(self, n_features):\n",
    "        super(NE_LR, self).__init__()\n",
    "        # the linear layer is the logistic regression model\n",
    "        # it takes n_features inputs and outputs 1 value\n",
    "        self.lr = torch.nn.Linear(n_features, 1)\n",
    "    \n",
    "    # pass data through the model and apply sigmoid activation\n",
    "    def forward(self, x):\n",
    "        output = torch.sigmoid(self.lr(x))\n",
    "        return output\n",
    "\n",
    "# Define the model, optimizer and loss function\n",
    "# Unencrypted training\n",
    "n_features = x_train.shape[1]\n",
    "model = NE_LR(n_features)\n",
    "# use gradient descent with a learning_rate=1\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1)\n",
    "# use Binary Cross Entropy Loss\n",
    "# BCELoss is the loss function used for binary classification\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# train the model for 5 epochs\n",
    "EPOCHS = 5\n",
    "# creating timimng list to store the time taken for each epoch\n",
    "times = []\n",
    "def train(model, optim, criterion, x, y, epochs=EPOCHS):\n",
    "    for e in range(1, epochs + 1):\n",
    "        start = time()\n",
    "        # set the gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # pass the data through the model\n",
    "        output = model(x)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optim.step()\n",
    "        end = time()\n",
    "        # loss is printed at each epoch\n",
    "        print(f\"Loss at epoch {e}: {loss.data:.4f}\")\n",
    "        times.append(end - start)\n",
    "    return model\n",
    "\n",
    "# Evaluate the model\n",
    "model = train(model, optim, criterion, x_train, y_train)\n",
    "#Calculating the accuracy of the model\n",
    "def accuracy(model, x, y):\n",
    "    out = model(x)\n",
    "    correct = torch.abs(y - out) < 0.5\n",
    "    return correct.float().mean()\n",
    "\n",
    "print(f\"\\nAverage time per epoch: {int(sum(times) / len(times))} seconds\")\n",
    "\n",
    "NE_accuracy = accuracy(model, x_test, y_test)\n",
    "print(f\"Non-Encrypted Accuracy: {NE_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncryptedLR:\n",
    "    # Encrypted Logistic Regression model    \n",
    "    def __init__(self, torch_lr):\n",
    "        # extract the weights and bias from the torch model\n",
    "        self.weight = torch_lr.lr.weight.data.tolist()[0]\n",
    "        # extract the bias from the torch model\n",
    "        self.bias = torch_lr.lr.bias.data.tolist()\n",
    "        #initialize the gradient accumulators and iterations count\n",
    "        self._delta_w = 0\n",
    "        self._delta_b = 0\n",
    "        self._count = 0\n",
    "    \n",
    "    #Forward pass\n",
    "    def forward(self, enc_x):\n",
    "        enc_out = enc_x.dot(self.weight) + self.bias\n",
    "        #Calculates linear combination of input and weight, adds bias\n",
    "        enc_out = EncryptedLR.sigmoid(enc_out)\n",
    "        #Applies sigmoid function\n",
    "        return enc_out\n",
    "    \n",
    "    #Backward pass\n",
    "    #Calculates the gradient of the loss w.r.t the weights and bias\n",
    "    def backward(self, enc_x, enc_out, enc_y):\n",
    "        out_minus_y = (enc_out - enc_y)\n",
    "        #Calculates the difference between the predicted value and the true value\n",
    "        self._delta_w += enc_x * out_minus_y\n",
    "        #Calculates the gradient of the loss w.r.t the weights\n",
    "        self._delta_b += out_minus_y\n",
    "        #Calculates the gradient of the loss w.r.t the bias\n",
    "        self._count += 1\n",
    "        #Increment the iteration count\n",
    "        \n",
    "    #Update the weights and bias\n",
    "    def update_parameters(self):\n",
    "        if self._count == 0:\n",
    "            raise RuntimeError(\"You should at least run one forward iteration\")\n",
    "        # update weights\n",
    "        # We use a small regularization term to keep the output\n",
    "        # of the linear layer in the range of the sigmoid approximation\n",
    "        self.weight -= self._delta_w * (1 / self._count) + self.weight * 0.05\n",
    "        self.bias -= self._delta_b * (1 / self._count)\n",
    "        # reset gradient accumulators and iterations count\n",
    "        self._delta_w = 0\n",
    "        self._delta_b = 0\n",
    "        self._count = 0\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(enc_x):\n",
    "        # sigmoid = 0.5 + 0.197 * x - 0.004 * x^3\n",
    "        # this is a degree 3 polynomial approximation of the sigmoid function\n",
    "        # it's used to keep the output of the linear layer in the range of the sigmoid approximation\n",
    "        return enc_x.polyval([0.5, 0.197, 0, -0.004])\n",
    "    \n",
    "    def plain_accuracy(self, x_test, y_test):\n",
    "    #Calculates the accuracy of the model on non-encrypted data\n",
    "        # convert the weights and bias to torch tensors\n",
    "        w = torch.tensor(self.weight)\n",
    "        b = torch.tensor(self.bias)\n",
    "        # pass the data through the linear layer\n",
    "        out = torch.sigmoid(x_test.matmul(w) + b).reshape(-1, 1)\n",
    "        # calculate the accuracy\n",
    "        correct = torch.abs(y_test - out) < 0.5\n",
    "        return correct.float().mean()    \n",
    "    \n",
    "    def encrypt(self, context):\n",
    "    #Encrypts the weights and bias\n",
    "        self.weight = ts.ckks_vector(context, self.weight)\n",
    "        self.bias = ts.ckks_vector(context, self.bias)\n",
    "        \n",
    "    def decrypt(self):\n",
    "    #Decrypts the weights and bias\n",
    "        self.weight = self.weight.decrypt()\n",
    "        self.bias = self.bias.decrypt()\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encryption of the training_set took 18 seconds\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "# the degree of the polynomial modulus\n",
    "poly_mod_degree = 8192\n",
    "# the bit-length of the modulus chain\n",
    "coeff_mod_bit_sizes = [40, 21, 21, 21, 21, 21, 21, 40]\n",
    "# create TenSEALContext\n",
    "enc_training = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
    "# generate keys\n",
    "enc_training.global_scale = 2 ** 21\n",
    "enc_training.generate_galois_keys()\n",
    "\n",
    "t_start = time()\n",
    "enc_x_train = [ts.ckks_vector(enc_training, x.tolist()) for x in x_train]\n",
    "enc_y_train = [ts.ckks_vector(enc_training, y.tolist()) for y in y_train]\n",
    "t_end = time()\n",
    "print(f\"Encryption of the training_set took {int(t_end - t_start)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at epoch #0 is 0.3392857015132904\n",
      "Accuracy at epoch #1 is 0.8482\n",
      "Loss at epoch #1 is 0.1518\n",
      "Accuracy at epoch #2 is 0.8839\n",
      "Loss at epoch #2 is 0.1161\n",
      "Accuracy at epoch #3 is 0.8527\n",
      "Loss at epoch #3 is 0.1473\n",
      "Accuracy at epoch #4 is 0.7991\n",
      "Loss at epoch #4 is 0.2009\n",
      "Accuracy at epoch #5 is 0.7589\n",
      "Loss at epoch #5 is 0.2411\n",
      "\n",
      "Average time per epoch: 62 seconds\n",
      "Accuracy 0.7589\n",
      "Difference between plain and encrypted accuracies: 0.0446\n"
     ]
    }
   ],
   "source": [
    "# create the encrypted model\n",
    "ELR = EncryptedLR(NE_LR(n_features))\n",
    "accuracy = ELR.plain_accuracy(x_test, y_test)\n",
    "print(f\"Accuracy at epoch #0 is {accuracy}\")\n",
    "# train the encrypted model\n",
    "times = []\n",
    "for epoch in range(EPOCHS):\n",
    "    ELR.encrypt(enc_training)\n",
    "    \n",
    "    t_start = time()\n",
    "    for enc_x, enc_y in zip(enc_x_train, enc_y_train):\n",
    "        # forward pass\n",
    "        enc_out = ELR.forward(enc_x)\n",
    "        # backward pass\n",
    "        ELR.backward(enc_x, enc_out, enc_y)\n",
    "    ELR.update_parameters()\n",
    "    t_end = time()\n",
    "    times.append(t_end - t_start)\n",
    "    # decrypt the model and calculate the accuracy\n",
    "    ELR.decrypt()\n",
    "    EN_accuracy = ELR.plain_accuracy(x_test, y_test)\n",
    "    print(f\"Accuracy at epoch #{epoch + 1} is {EN_accuracy:.4f}\")\n",
    "    print(f\"Loss at epoch #{epoch + 1} is {(1 - EN_accuracy):.4f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAverage time per epoch: {int(sum(times) / len(times))} seconds\")\n",
    "print(f\"Accuracy {EN_accuracy:.4f}\")\n",
    "\n",
    "diff_accuracy = NE_accuracy - EN_accuracy\n",
    "print(f\"Difference between plain and encrypted accuracies: {diff_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVvid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
